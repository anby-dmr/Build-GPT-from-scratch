{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of chars 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"num of chars\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# First 1000 chars\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# All unique chars that occur\n",
    "# We'all use char as token\n",
    "chars = sorted(list(set(text)))\n",
    "print(''.join(chars))\n",
    "print(len(chars))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 56, 53, 51, 1, 43, 47, 58, 46, 43, 56, 1, 57, 47, 42, 43, 57, 1, 58, 46, 43, 1, 56, 47, 60, 43, 56, 1, 50, 47, 43, 57]\n",
      "['F', 'r', 'o', 'm', ' ', 'e', 'i', 't', 'h', 'e', 'r', ' ', 's', 'i', 'd', 'e', 's', ' ', 't', 'h', 'e', ' ', 'r', 'i', 'v', 'e', 'r', ' ', 'l', 'i', 'e', 's']\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenizer example (just using index) (More complex, see OpenAI tiktoken)\n",
    "# Write the encode and decode function\n",
    "stoi = {character:index for index, character in enumerate(chars)}\n",
    "itos = {index:character for index, character in enumerate(chars)}\n",
    "encode = lambda chars: [stoi[x] for x in chars]\n",
    "decode = lambda ints: [itos[x] for x in ints]\n",
    "\n",
    "test_str = \"From either sides the river lies\"\n",
    "test_str_code = encode(test_str)\n",
    "test_str_decode = decode(test_str_code)\n",
    "print(test_str_code)\n",
    "print(test_str_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和测试集、批量抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire dataset\n",
    "# Split the train and validation set of the dataset\n",
    "# Transfer to tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "split_point = int(0.9 * len(data))\n",
    "train_data = data[:split_point]\n",
    "test_data = data[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 61, 39, 47, 50,  1, 47, 58],\n",
      "        [50, 53, 60, 43,  6,  1, 58, 53],\n",
      "        [43,  1, 58, 46, 43,  1, 41, 47],\n",
      "        [61, 53, 56, 42, 57,  1, 47, 52],\n",
      "        [43, 41, 46,  1, 63, 53, 59,  2],\n",
      "        [44, 59, 50,  1, 39, 42, 60, 43],\n",
      "        [43,  1, 39, 52, 42,  1, 51, 63],\n",
      "        [43, 52, 42, 43, 56,  6,  1, 51]])\n"
     ]
    }
   ],
   "source": [
    "# Write the get_batch function\n",
    "# Draw chunks of data and understand how to use a chunk of data consider varying length\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_batch(data):\n",
    "    indices = torch.randint(len(data)-block_size, (batch_size, ))\n",
    "    x = [data[start:start+block_size] for start in indices]\n",
    "    y = [data[start+1:start+block_size+1] for start in indices]\n",
    "    x, y = torch.stack(x), torch.stack(y)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(train_data)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Training input is  tensor([18])\n",
      "Target is  tensor(47)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47])\n",
      "Target is  tensor(56)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56])\n",
      "Target is  tensor(57)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57])\n",
      "Target is  tensor(58)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58])\n",
      "Target is  tensor(1)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1])\n",
      "Target is  tensor(15)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15])\n",
      "Target is  tensor(47)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "Target is  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "# The way how a chunk of data is used: Enumerate all possible prediction context length\n",
    "x_example = train_data[:block_size]\n",
    "y_example = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    x = x_example[:t+1]\n",
    "    y = y_example[t]\n",
    "    print(\"----------------------\")\n",
    "    print(\"Training input is \", x)\n",
    "    print(\"Target is \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单BigramLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 65])\n",
      "tensor(4.6977, grad_fn=<NllLossBackward0>)\n",
      " secret:\n",
      " secret:s;jfxAbE\n"
     ]
    }
   ],
   "source": [
    "# Test the data using simple language model BigramLanguageModel\n",
    "# Embedding in Bigram is just a nxn matrix represents the transition probability from token to token\n",
    "\"\"\"\n",
    "BigramLanguageModel\n",
    "-------------------\n",
    "Forward:\n",
    "Input: x, tensor of training data with shape (B, T).\n",
    "Input: target, tensor of labeling data with shape (B, T). Default target = None\n",
    "Output: logits, rows of the probability of each token in the data x, shape (B, T, C), C is the length of the vocab.\n",
    "Output: loss, cross entropy of logits and targets. Note that input logits have to be reshaped to use crossEntropyLoss.\n",
    "\n",
    "Generate: write a generic version that considers the history\n",
    "Input: x, tensor (B, T)\n",
    "Input: max_new_tokens\n",
    "Output: x' after expanding\n",
    "\"\"\"\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        logits = self.token_embedding_table(x)\n",
    "        \n",
    "        if target is None:\n",
    "            loss = 0\n",
    "        else:\n",
    "            B, T, C = logits.shape # C represents channels\n",
    "            logits_reshaped = logits.view(B*T, C)\n",
    "            target = target.view(-1)\n",
    "            loss = F.cross_entropy(logits_reshaped, target)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, predict_len):\n",
    "        for step in range(predict_len):\n",
    "            logits, _ = self(x)\n",
    "            logits = logits[:, -1, :] # Draw the logits of last time step. Now the shape is (B, C)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            char_new = torch.multinomial(probs, 1)\n",
    "            x = torch.cat([x, char_new], dim=1)\n",
    "        return x\n",
    "\n",
    "model = BigramLanguageModel(len(chars))\n",
    "model = model.to(device)\n",
    "x, y = get_batch(train_data)\n",
    "logits, loss = model(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# Use .tolist() method to transform the tensor\n",
    "print(''.join(decode(x[0].tolist())))\n",
    "x_predicted = model.generate(x, 8)\n",
    "print(''.join(decode(x_predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1431.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8574, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Bigram model\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1000\n",
    "for _ in tqdm(range(epochs)):\n",
    "    x, y = get_batch(train_data)\n",
    "    _, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True) # Save memory, and faster\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\n",
      "Peace\n",
      "I:\n",
      "PeaceXAg;bPxU$$ZzmrcYV'C-qjymZkBZftRtAFf'hensoubvYwD?DPzm!zU'x3 JuTO\n",
      ":SCKeRyUER3dl.HRCilj,QedqATcbbfl!xtw\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(train_data)\n",
    "\n",
    "# Use .tolist() method to transform the tensor\n",
    "print(''.join(decode(x[0].tolist())))\n",
    "x_predicted = model.generate(x, 100)\n",
    "print(''.join(decode(x_predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算attention的向量化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-2.1055,  0.6784, -1.2345],\n",
      "        [-0.0431, -1.6047, -0.7521],\n",
      "        [ 1.6487, -0.3925, -1.4036],\n",
      "        [-0.7279, -0.5594, -0.7688]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# For a (B, T, C) tensor\n",
    "# By averaging over the prefix tokens to summarize the past info and ignore the future info (just like autoregressive)\n",
    "# Ver1: Double loops version\n",
    "# Track the dim can help coding\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "B = 4\n",
    "T = 5\n",
    "C = 3\n",
    "x = torch.randn((B, T, C))\n",
    "\n",
    "print(x[0])\n",
    "\n",
    "x_loop = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        x_loop[b][t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x_loop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# Ver2: Matrix mul version. Efficient, using lower triangle torch.tril(torch.ones(3, 3)) and normalize it\n",
    "# Use torch.allclose() to check these two\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print(wei)\n",
    "x_MM = wei @ x\n",
    "print(x_MM[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# Ver3: Set -inf (it is a mask) and then softmax to get the normalized lower triangular\n",
    "# Why use this? A perspective of affinities. Weight matrix can represent how each token is interesting to each other (In this example it's just 0)\n",
    "# Use [masked_fill] method\n",
    "wei = torch.zeros((T, T))\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, value=-float('inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "x_sm = wei @ x\n",
    "print(x_sm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1491, 0.8509, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6266, 0.0590, 0.3144, 0.0000, 0.0000],\n",
      "        [0.2801, 0.0278, 0.2398, 0.4523, 0.0000],\n",
      "        [0.2760, 0.1401, 0.2008, 0.2031, 0.1801]], grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.5241,  1.4449,  1.0499, -1.2772, -0.4092],\n",
      "        [-0.4937, -0.3647, -1.0037,  1.0804,  0.6383],\n",
      "        [ 0.3009, -0.1853, -0.4107,  1.5519, -1.2764],\n",
      "        [ 0.2153,  1.2557,  0.0874,  0.4091, -1.5597],\n",
      "        [ 0.0270, -0.1853, -0.5358,  1.1992, -0.4808]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.5241,  1.4449,  1.0499, -1.2772, -0.4092],\n",
      "        [-0.3419, -0.0948, -0.6974,  0.7288,  0.4821],\n",
      "        [ 0.3938,  0.8256,  0.4695, -0.2486, -0.6200],\n",
      "        [ 0.3026,  0.9182,  0.2073,  0.2294, -1.1084],\n",
      "        [ 0.1845,  0.5321, -0.0121,  0.4095, -0.6831]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Ver4: Self-attention\n",
    "# query * key to get the weights that represents the affinities(relationship) from token to token\n",
    "# When training, the model will automatically learn the affinities from token to token.\n",
    "# A simple one head attention.\n",
    "\n",
    "\"\"\"\n",
    "Important notes about atttention:\n",
    "1. a communication machanism that can be describe by direct graph with block_size node (the edge represents the affinities.\n",
    "    not necessarily to be a lower triangle. Encoder part needs tokens to talk to each other.\n",
    "2. no notion of space. positional code is needed.\n",
    "3. Raw value x is like private data, and V is like public data for aggregation\n",
    "4. Self-attention: QKV from same source x. Cross-attention: outer source QK.\n",
    "\"\"\"\n",
    "torch.manual_seed(42)\n",
    "head_size = 5\n",
    "Q = nn.Linear(C, head_size)\n",
    "K = nn.Linear(C, head_size)\n",
    "V = nn.Linear(C, head_size)\n",
    "\n",
    "q = Q(x) # (B, T, head_size)\n",
    "k = K(x)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, value=-float('inf')) * head_size**(-0.5)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei[0])\n",
    "\n",
    "v = V(x)\n",
    "print(v[0])\n",
    "x_att = wei @ v # A good way of thinking this mat mul is row perspective\n",
    "print(x_att[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# Why scaling using query and key dimension matters? \n",
    "# We wants the attention to combine more info. If not scaling, it will tend to focus on the largest one.\n",
    "# Essentially, by scalling, the final weight matrix will have low variance.\n",
    "example = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "print(torch.softmax(example, dim=-1))\n",
    "print(torch.softmax(example * 8, dim=-1)) # Not what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Implement a self-attention Head: qkv layer, tril register_buffer, dropout_layer\n",
    "\"\"\"\n",
    "Input: (B, T, embed_size) data, head_size\n",
    "Output: (B, T, head_size)\n",
    "\n",
    "- Note that we add dropout for weight matrix to randomly prevent some interaction.\n",
    "\n",
    "? Why use C**(-0.5) instead of head_size**(-0.5)? I think the head_size it the right choice.\n",
    "? Why slicing the tril matrix\n",
    "\"\"\"\n",
    "\n",
    "embed_size = 8\n",
    "dropout = 0.1\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False) # qkv usually not use bias.\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        wei = q @ k.transpose(-1, -2) * self.head_size**(-0.5) # Guess this should be head_size instead of C according to the paper.\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, value=-float('inf')) # Careful! Slice the tril. Because during inference, the length of data may change.\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        b = wei @ v # (B, T, head_size)\n",
    "        return b\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "att_head = Head(head_size=8)\n",
    "b = att_head(x)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Implement a multi-head attention with projection (Linear trans)\n",
    "# Projection layer here is to map the size back to embeding_size, which is then compatible for residual connetion\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        b = self.proj(b)\n",
    "        b = self.dropout(b)\n",
    "        return b\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "multi_att_head = MultiHeadAttention(num_heads=4, head_size=embed_size//4)\n",
    "b = multi_att_head(x)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement the feed forward networks after attention layer\n",
    "- Note that we add dropout before the res connection.\n",
    "\"\"\"\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implement attention block class with residual connection\n",
    "- Note that we choose num_heads as hyper parameter instead of choosing head_size. The head_size is computed from embed_size//num_heads\n",
    "- Note that we usually use layerNorm before att and fc now, which is opposite from the original paper.\n",
    "? Why choose layer norm? And how to use LayerNorm of 1d and 2d.\n",
    "\"\"\"\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // num_heads       \n",
    "        self.att = MultiHeadAttention(num_heads, head_size)\n",
    "        self.fc = FeedForward(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.fc(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "block = Block(embed_size, num_heads=4)\n",
    "print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention用于语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modification on BigramModel -- 3 Linear layers\n",
    "1. token_embedding layer (vocab_size, embed_size)\n",
    "2. position_embedding layer (block_size, embed_size)\n",
    "3. language model head used to compute logits (embed_size, block_size)\n",
    "4. Pluggin the attention block (use multi-head directly)\n",
    "5. Pluggin the last feedforward layer\n",
    "\n",
    "Make sure you understand the embedding here, which is no longer the same as original BigrameModel (which represents the transition probability)\n",
    "    nn.Embedding creates a lookup table that converts indices (usually token IDs) into dense vectors of fixed size. \n",
    "    It's commonly used as the first layer in NLP tasks to convert tokens to continuous representations.\n",
    "So this model is actually not a BigramModel anymore.\n",
    "\n",
    "Just to remind1: loss is computed through cross_entropy. before using F.cross_entropy, you only need to reshape the logits and target.\n",
    "Just to remind2: use F.softmax to convert the logits to the probability and then use torch.multinomial to predict the next token.\n",
    "\n",
    "? Why token_embed and pos_embed use nn.Embedding instead of nn.Linear\n",
    "? Understand the embedding in NLP tasks, why it is necessary.\n",
    "\"\"\"\n",
    "\n",
    "num_layers = 6\n",
    "num_heads = 4\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embed = nn.Embedding(block_size, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block(embed_size, num_heads) for _ in range(num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok_emb = self.token_embed(x) # (B, T, C)\n",
    "        pos_emb = self.pos_embed(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits_temp = logits.reshape(B*T, C)\n",
    "            target_temp = target.reshape(B*T)\n",
    "            loss = F.cross_entropy(logits_temp, target_temp)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, x, num_predict):\n",
    "        for t in range(num_predict):\n",
    "            x_cond = x[:,-block_size:] # (B, T)\n",
    "            logits, _ = self(x_cond) # (B, T, vocab_size)\n",
    "            logits = logits[:,-1,:] # (B, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
    "            x_next = torch.multinomial(probs, 1) # (B, 1)\n",
    "            x = torch.cat([x, x_next], dim=1) # (B, T+1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context test after training in gpt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M params\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_iters = 200\n",
    "lr = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embed_size = 64\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "dropout = 0.0\n",
    "\n",
    "model = BigramModel()\n",
    "model = model.to(device)\n",
    "# Print the num of parameters\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M params')\n",
    "model.load_state_dict(torch.load('GPT4999.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "AUFIDIUS:\n",
      "A foust plaxcion.\n",
      "\n",
      "GRIAHNIUS:\n",
      "God'd your worth oar?\n",
      "\n",
      "LUCESTION:\n",
      "No day, the bots, and with'rt gift\n",
      "Is the must fields, as I\n",
      "Hade ear, si ruitom for goat did to Vimendins hess all\n",
      "Have to were the be\n",
      "Thatgen a be doth our.\n",
      "\n",
      "PENTESBY:\n",
      "Stresh OMERDIODY ANNNE:\n",
      "Gike't, of it: yet shell bolaughting:\n",
      "Vathers I anst holt Richard abonless\n",
      "Thou come. Noquithing the you to those Tent and bother quadst: for with madam?\n",
      "Thol cortumen, let.\n",
      "\n",
      "SAMILLO:\n",
      "A bolding I heaven, for fairend.\n",
      "I show Think you art of she, busimpe you, flo-moran, shall.\n",
      "\n",
      "FRGHEv OF YORK:\n",
      "Laddounds of you state pasp, whereford not, deave\n",
      "That I she hall that oither!\n",
      "Iatinness, thou what rells bresing for Neforc's this in againtht\n",
      "with doubt. Go madam weell, godks it.\n",
      "\n",
      "POMERSETES:\n",
      "Thendelly strutht is whither pardure be they for doath:' against thus as liss a mocking to with with,\n",
      "The he tou asst be boinds again.\n",
      "\n",
      "BRUTUM:\n",
      "My so for I soul I nobly me thou your plawn?\n",
      "-prespock on is of the may tle preast.\n",
      "\n",
      "CPARGARET:\n",
      "Lord Aukel, this.\n",
      "Ay, peall to bound sto? there of his heart:\n",
      "He stimp you hasts, I maid this prince this.\n",
      "\n",
      "POPRIOL:\n",
      "I kful paying not this my hights,\n",
      "This atmost the he is not is or man I'll get preson;\n",
      "And thou nexst, and ne, upon bloody ded to dedsir,\n",
      "That Dearly angord Lord?\n",
      "\n",
      "My his AUTis I mabbs Lort of\n",
      "That oind Gloyard, that dember, he no, I hasdive the prome!\n",
      "I murdo.\n",
      "\n",
      "TJUS:\n",
      "Affienot's of mark.\n",
      "\n",
      "NORY CAPUT: thou nor boundss!\n",
      "Thesp-divid I kne't on their broad. Magramour:\n",
      "Till very hip agow carina: if as 's against his tongue, I fair thy are out becriss;\n",
      "For beenss, sinvolst of what me prince of Ise but I\n",
      "the fair he his mote mother in of lovest\n",
      "And The copbeture his fand;\n",
      "Thi no madam that will!\n",
      "O yes, grieful his maid wrong brother how\n",
      "Ox s these his must again, fave swears love nexss objeitys patter\n",
      "Stake lord: yet me? looks in't?\n",
      "\n",
      "LADY GEnnown you know strathward my n tose,\n",
      "I those to behing tillend, y'll night,\n",
      "no bigk his is a klland me put bost;\n",
      "Vestrich her scoof your as ac\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # Input (B=1, T=1)\n",
    "print(''.join(decode(model.generate(context, 2000)[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试实现LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更多细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add dropout and layerNorm only when you're ready to scal up the model to prevent overfitting.\n",
    "2. Why decoder only? Because we perform non-conditional generation. No needs for encoder. (The original paper perform machine translation, which is a conditional generation task)\n",
    "3. Mask in decoder make the model autoregressive.\n",
    "4. A more efficient way: Causal self-attention, which incorporate the multi-head into a dimension of the data, making it (B, T, C, H).\n",
    "5. Two phases of training a LLM: (1) Document completer: pre-train on large internet data (just like what we did here). (2) QA assistant: fine-tuning on the data of QA format, and train a reward model then do RLHF. This stage is harder and a lot of data is closed-source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
