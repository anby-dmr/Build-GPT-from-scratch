{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of chars 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"num of chars\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# First 1000 chars\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# All unique chars that occur\n",
    "# We'all use char as token\n",
    "chars = sorted(list(set(text)))\n",
    "print(''.join(chars))\n",
    "print(len(chars))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 56, 53, 51, 1, 43, 47, 58, 46, 43, 56, 1, 57, 47, 42, 43, 57, 1, 58, 46, 43, 1, 56, 47, 60, 43, 56, 1, 50, 47, 43, 57]\n",
      "['F', 'r', 'o', 'm', ' ', 'e', 'i', 't', 'h', 'e', 'r', ' ', 's', 'i', 'd', 'e', 's', ' ', 't', 'h', 'e', ' ', 'r', 'i', 'v', 'e', 'r', ' ', 'l', 'i', 'e', 's']\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenizer example (just using index) (More complex, see OpenAI tiktoken)\n",
    "# Write the encode and decode function\n",
    "stoi = {character:index for index, character in enumerate(chars)}\n",
    "itos = {index:character for index, character in enumerate(chars)}\n",
    "encode = lambda chars: [stoi[x] for x in chars]\n",
    "decode = lambda ints: [itos[x] for x in ints]\n",
    "\n",
    "test_str = \"From either sides the river lies\"\n",
    "test_str_code = encode(test_str)\n",
    "test_str_decode = decode(test_str_code)\n",
    "print(test_str_code)\n",
    "print(test_str_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和测试集、批量抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire dataset\n",
    "# Split the train and validation set of the dataset\n",
    "# Transfer to tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "split_point = int(0.9 * len(data))\n",
    "train_data = data[:split_point]\n",
    "test_data = data[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17, 10,  0, 35, 46, 43, 56, 43],\n",
      "        [51,  6,  1, 47, 52,  1, 46, 53],\n",
      "        [63, 53, 59, 56,  1, 42, 59, 58],\n",
      "        [12,  1, 58, 46, 43, 52,  6,  1],\n",
      "        [46, 39, 58, 43,  1, 53, 44,  1],\n",
      "        [ 1, 45, 53, 58,  1, 53, 44, 44],\n",
      "        [ 1, 58, 46, 47, 57,  1, 52, 47],\n",
      "        [43, 39, 50, 51,  1, 53, 44,  1]])\n"
     ]
    }
   ],
   "source": [
    "# Write the get_batch function\n",
    "# Draw chunks of data and understand how to use a chunk of data consider varying length\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_batch(data):\n",
    "    indices = torch.randint(len(data)-block_size, (batch_size, ))\n",
    "    x = [data[start:start+block_size] for start in indices]\n",
    "    y = [data[start+1:start+block_size+1] for start in indices]\n",
    "    x, y = torch.stack(x), torch.stack(y)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(train_data)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Training input is  tensor([18])\n",
      "Target is  tensor(47)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47])\n",
      "Target is  tensor(56)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56])\n",
      "Target is  tensor(57)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57])\n",
      "Target is  tensor(58)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58])\n",
      "Target is  tensor(1)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1])\n",
      "Target is  tensor(15)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15])\n",
      "Target is  tensor(47)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "Target is  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "# The way how a chunk of data is used: Enumerate all possible prediction context length\n",
    "x_example = train_data[:block_size]\n",
    "y_example = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    x = x_example[:t+1]\n",
    "    y = y_example[t]\n",
    "    print(\"----------------------\")\n",
    "    print(\"Training input is \", x)\n",
    "    print(\"Target is \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单BigramLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 65])\n",
      "tensor(4.5156, grad_fn=<NllLossBackward0>)\n",
      "ng of th\n",
      "ng of thm'Y?:uPj\n"
     ]
    }
   ],
   "source": [
    "# Test the data using simple language model BigramLanguageModel\n",
    "# Embedding in Bigram is just a nxn matrix represents the transition probability from token to token\n",
    "\"\"\"\n",
    "BigramLanguageModel\n",
    "-------------------\n",
    "Forward:\n",
    "Input: x, tensor of training data with shape (B, T).\n",
    "Input: target, tensor of labeling data with shape (B, T). Default target = None\n",
    "Output: logits, rows of the probability of each token in the data x, shape (B, T, C), C is the length of the vocab.\n",
    "Output: loss, cross entropy of logits and targets. Note that input logits have to be reshaped to use crossEntropyLoss.\n",
    "\n",
    "Generate: write a generic version that considers the history\n",
    "Input: x, tensor (B, T)\n",
    "Input: max_new_tokens\n",
    "Output: x' after expanding\n",
    "\"\"\"\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        logits = self.token_embedding_table(x)\n",
    "        \n",
    "        if target is None:\n",
    "            loss = 0\n",
    "        else:\n",
    "            B, T, C = logits.shape # C represents channels\n",
    "            logits_reshaped = logits.view(B*T, C)\n",
    "            target = target.view(-1)\n",
    "            loss = F.cross_entropy(logits_reshaped, target)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, predict_len):\n",
    "        for step in range(predict_len):\n",
    "            logits, _ = self(x)\n",
    "            logits = logits[:, -1, :] # Draw the logits of last time step. Now the shape is (B, C)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            char_new = torch.multinomial(probs, 1)\n",
    "            x = torch.cat([x, char_new], dim=1)\n",
    "        return x\n",
    "\n",
    "model = BigramLanguageModel(len(chars))\n",
    "model = model.to(device)\n",
    "x, y = get_batch(train_data)\n",
    "logits, loss = model(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# Use .tolist() method to transform the tensor\n",
    "print(''.join(decode(x[0].tolist())))\n",
    "x_predicted = model.generate(x, 8)\n",
    "print(''.join(decode(x_predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1523.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8277, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Bigram model\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1000\n",
    "for _ in tqdm(range(epochs)):\n",
    "    x, y = get_batch(train_data)\n",
    "    _, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True) # Save memory, and faster\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for mone\n",
      "for mone,tiPwTGuhZ\n",
      ",fsWuncZh.mZCZCXGBLBCwMqrUWBce-iOy,tWJlM;uhbrK!y?I:3ZLR,ttse?ZG;qvahLZGqGh33qqjaC?ipqJSJk\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(train_data)\n",
    "\n",
    "# Use .tolist() method to transform the tensor\n",
    "print(''.join(decode(x[0].tolist())))\n",
    "x_predicted = model.generate(x, 100)\n",
    "print(''.join(decode(x_predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算attention的向量化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-2.1055,  0.6784, -1.2345],\n",
      "        [-0.0431, -1.6047, -0.7521],\n",
      "        [ 1.6487, -0.3925, -1.4036],\n",
      "        [-0.7279, -0.5594, -0.7688]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# For a (B, T, C) tensor\n",
    "# By averaging over the prefix tokens to summarize the past info and ignore the future info (just like autoregressive)\n",
    "# Ver1: Double loops version\n",
    "# Track the dim can help coding\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "B = 4\n",
    "T = 5\n",
    "C = 3\n",
    "x = torch.randn((B, T, C))\n",
    "\n",
    "print(x[0])\n",
    "\n",
    "x_loop = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        x_loop[b][t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x_loop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# Ver2: Matrix mul version. Efficient, using lower triangle torch.tril(torch.ones(3, 3)) and normalize it\n",
    "# Use torch.allclose() to check these two\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print(wei)\n",
    "x_MM = wei @ x\n",
    "print(x_MM[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# Ver3: Set -inf (it is a mask) and then softmax to get the normalized lower triangular\n",
    "# Why use this? A perspective of affinities. Weight matrix can represent how each token is interesting to each other (In this example it's just 0)\n",
    "# Use [masked_fill] method\n",
    "wei = torch.zeros((T, T))\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, value=-float('inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "x_sm = wei @ x\n",
    "print(x_sm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1491, 0.8509, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6266, 0.0590, 0.3144, 0.0000, 0.0000],\n",
      "        [0.2801, 0.0278, 0.2398, 0.4523, 0.0000],\n",
      "        [0.2760, 0.1401, 0.2008, 0.2031, 0.1801]], grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.5241,  1.4449,  1.0499, -1.2772, -0.4092],\n",
      "        [-0.4937, -0.3647, -1.0037,  1.0804,  0.6383],\n",
      "        [ 0.3009, -0.1853, -0.4107,  1.5519, -1.2764],\n",
      "        [ 0.2153,  1.2557,  0.0874,  0.4091, -1.5597],\n",
      "        [ 0.0270, -0.1853, -0.5358,  1.1992, -0.4808]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.5241,  1.4449,  1.0499, -1.2772, -0.4092],\n",
      "        [-0.3419, -0.0948, -0.6974,  0.7288,  0.4821],\n",
      "        [ 0.3938,  0.8256,  0.4695, -0.2486, -0.6200],\n",
      "        [ 0.3026,  0.9182,  0.2073,  0.2294, -1.1084],\n",
      "        [ 0.1845,  0.5321, -0.0121,  0.4095, -0.6831]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Ver4: Self-attention\n",
    "# query * key to get the weights that represents the affinities(relationship) from token to token\n",
    "# When training, the model will automatically learn the affinities from token to token.\n",
    "# A simple one head attention.\n",
    "\n",
    "\"\"\"\n",
    "Important notes about atttention:\n",
    "1. a communication machanism that can be describe by direct graph with block_size node (the edge represents the affinities.\n",
    "    not necessarily to be a lower triangle. Encoder part needs tokens to talk to each other.\n",
    "2. no notion of space. positional code is needed.\n",
    "3. Raw value x is like private data, and V is like public data for aggregation\n",
    "4. Self-attention: QKV from same source x. Cross-attention: outer source QK.\n",
    "\"\"\"\n",
    "torch.manual_seed(42)\n",
    "head_size = 5\n",
    "Q = nn.Linear(C, head_size)\n",
    "K = nn.Linear(C, head_size)\n",
    "V = nn.Linear(C, head_size)\n",
    "\n",
    "q = Q(x) # (B, T, head_size)\n",
    "k = K(x)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, value=-float('inf')) * head_size**(-0.5)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei[0])\n",
    "\n",
    "v = V(x)\n",
    "print(v[0])\n",
    "x_att = wei @ v # A good way of thinking this mat mul is row perspective\n",
    "print(x_att[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# Why scaling using query and key dimension matters? \n",
    "# We wants the attention to combine more info. If not scaling, it will tend to focus on the largest one.\n",
    "# Essentially, by scalling, the final weight matrix will have low variance.\n",
    "example = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "print(torch.softmax(example, dim=-1))\n",
    "print(torch.softmax(example * 8, dim=-1)) # Not what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Implement a self-attention Head: qkv layer, tril register_buffer, dropout_layer\n",
    "\"\"\"\n",
    "Input: (B, T, embed_size) data, head_size\n",
    "Output: (B, T, head_size)\n",
    "\n",
    "? Why use C**(-0.5) instead of head_size**(-0.5)\n",
    "? Why slicing the tril matrix\n",
    "\"\"\"\n",
    "\n",
    "embed_size = 8\n",
    "dropout = 0.1\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False) # qkv usually not use bias.\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        wei = q @ k.transpose(-1, -2) * C**(-0.5) # Careful! Use C**(-0.5) instead of head_size**(-0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, value=-float('inf')) # Careful! Slice the tril.\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        b = wei @ v # (B, T, head_size)\n",
    "        return b\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "att_head = Head(head_size=8)\n",
    "b = att_head(x)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Implement a multi-head attention with projection (Linear trans)\n",
    "# Projection layer here is to map the size back to embeding_size, which is then compatible for residual connetion\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        b = self.proj(b)\n",
    "        b = self.dropout(b)\n",
    "        return b\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "multi_att_head = MultiHeadAttention(num_heads=4, head_size=embed_size//4)\n",
    "b = multi_att_head(x)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement the feed forward networks after attention layer\n",
    "\"\"\"\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implement attention block class with residual connection\n",
    "- Note that we choose num_heads as hyper parameter instead of choosing head_size. The head_size is computed from embed_size//num_heads\n",
    "? Why choose layer norm? And how to use LayerNorm of 1d and 2d.\n",
    "\"\"\"\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // num_heads       \n",
    "        self.att = MultiHeadAttention(num_heads, head_size)\n",
    "        self.fc = FeedForward(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.ln1(self.att(x))\n",
    "        x = x + self.ln2(self.fc(x))\n",
    "        return x\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "block = Block(embed_size, num_heads=4)\n",
    "print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention用于语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modification on BigramModel -- 3 Linear layers\n",
    "1. token_embedding layer (vocab_size, embed_size)\n",
    "2. position_embedding layer (block_size, embed_size)\n",
    "3. language model head used to compute logits (embed_size, block_size)\n",
    "4. Pluggin the attention block (use multi-head directly)\n",
    "5. Pluggin the last feedforward layer\n",
    "\n",
    "Make sure you understand the embedding here, which is no longer the same as original BigrameModel (which represents the transition probability)\n",
    "    nn.Embedding creates a lookup table that converts indices (usually token IDs) into dense vectors of fixed size. \n",
    "    It's commonly used as the first layer in NLP tasks to convert tokens to continuous representations.\n",
    "So this model is actually not a BigramModel anymore.\n",
    "\n",
    "Just to remind1: loss is computed through cross_entropy. before using F.cross_entropy, you only need to reshape the logits and target.\n",
    "Just to remind2: use F.softmax to convert the logits to the probability and then use torch.multinomial to predict the next token.\n",
    "\n",
    "? Why token_embed and pos_embed use nn.Embedding instead of nn.Linear\n",
    "? Understand the embedding in NLP tasks, why it is necessary.\n",
    "\"\"\"\n",
    "\n",
    "num_layers = 6\n",
    "num_heads = 4\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embed = nn.Embedding(block_size, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block(embed_size, num_heads) for _ in range(num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok_emb = self.token_embed(x) # (B, T, C)\n",
    "        pos_emb = self.pos_embed(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits_temp = logits.reshape(B*T, C)\n",
    "            target_temp = target.reshape(B*T)\n",
    "            loss = F.cross_entropy(logits_temp, target_temp)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, x, num_predict):\n",
    "        for t in range(num_predict):\n",
    "            x_cond = x[:,-block_size:] # (B, T)\n",
    "            logits, _ = self(x_cond) # (B, T, vocab_size)\n",
    "            logits = logits[:,-1,:] # (B, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
    "            x_next = torch.multinomial(probs, 1) # (B, 1)\n",
    "            x = torch.cat([x, x_next], dim=1) # (B, T+1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context test after training in gpt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M params\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_iters = 200\n",
    "lr = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embed_size = 64\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "dropout = 0.0\n",
    "\n",
    "model = BigramModel()\n",
    "model = model.to(device)\n",
    "# Print the num of parameters\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M params')\n",
    "model.load_state_dict(torch.load('GPT4999.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RICHARD IIIIII:\n",
      "\n",
      "LINGOONUCESTESS:\n",
      "This alls pit's prany-set him die!\n",
      "Few that poor you ardisfot, and,'\n",
      "And a hall inthis soast a say iI drince a been\n",
      "This a fool other make, my look andor a be grace:\n",
      "Thy fiond; and be lies powers make\n",
      "At be nefriencel this pilt\n",
      "That of be such your call to your baing in procent\n",
      "Whently OF AUMEROY:\n",
      "Or. The king's ir mother, thou since more endeed?\n",
      "\n",
      "First Long it; the get a guarder's the by will warrake's sought of\n",
      "Their atroke of the disedy medans of my crossings traciousand.\n",
      "Ah prop, and whose show's see, be speak.\n",
      "Come, go you have I sound, who heard\n",
      "These colding agoraness and my on they sound: face the one life my my wongel away;\n",
      "Not would\n",
      "Upon and Citizenable see vicisce!\n",
      "\n",
      "Messed Mirchalies Audial she!\n",
      "Shall him both fair aglace a but be by-partss,\n",
      "Norse, a'ds bother for beness,\n",
      "A holling me--\n",
      "\n",
      "Nurse:\n",
      "In heaving tended, the down.\n",
      "\n",
      "JURCHIOH:\n",
      "You mist requick must call atters i' swifed be if neved\n",
      "When\n",
      "say,\n",
      "No whose we how with i' the felsesure conster that, my shoops;\n",
      "Pray me he's entn's comque be son,\n",
      "That Daking up mine desist for my let go for faith.\n",
      "\n",
      "LEONTES:\n",
      "Cither, 'tis caushable't this spobs\n",
      "Though it hosman, and but thou must the be but meast not\n",
      "Not more his to poison\n",
      "his might, set you to dist weeph aDe power bake us a bungeration\n",
      "I put let unread's friend\n",
      "theil out have be sad it a endumman.\n",
      "\n",
      "GREGHESTER:\n",
      "Say you his bajest another Xoldow,\n",
      "Too stright of curge commor possay;\n",
      "Till aday untito the will god?\n",
      "\n",
      "MONTAGANUA:\n",
      "Fiver the gaive find; yet this we prose.\n",
      "That cappetion. I had night, by part weell meets,\n",
      "The spokeo at himbsed will thy for be a far thought to mine\n",
      "The main; noblemt be our ray, stongey\n",
      "love our you may of ne.\n",
      "\n",
      "First Morne:\n",
      "That of thy defair suit shout not bey you gailts?\n",
      "Thou be I my lords, so you wand he Amake this and bitch I we home brincal offord his we our comes\n",
      "Uposives into then post as mean I treven EDYnel:\n",
      "I possue this is seasure queence.\n",
      "Tike no, that I may premakent bother\n",
      "Thannow against\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # Input (B=1, T=1)\n",
    "print(''.join(decode(model.generate(context, 2000)[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
