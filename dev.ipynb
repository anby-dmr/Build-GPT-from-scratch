{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of chars 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"num of chars\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# First 1000 chars\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# All unique chars that occur\n",
    "# We'all use char as token\n",
    "chars = sorted(list(set(text)))\n",
    "print(''.join(chars))\n",
    "print(len(chars))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 56, 53, 51, 1, 43, 47, 58, 46, 43, 56, 1, 57, 47, 42, 43, 57, 1, 58, 46, 43, 1, 56, 47, 60, 43, 56, 1, 50, 47, 43, 57]\n",
      "['F', 'r', 'o', 'm', ' ', 'e', 'i', 't', 'h', 'e', 'r', ' ', 's', 'i', 'd', 'e', 's', ' ', 't', 'h', 'e', ' ', 'r', 'i', 'v', 'e', 'r', ' ', 'l', 'i', 'e', 's']\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenizer example (just using index) (More complex, see OpenAI tiktoken)\n",
    "# Write the encode and decode function\n",
    "stoi = {character:index for index, character in enumerate(chars)}\n",
    "itos = {index:character for index, character in enumerate(chars)}\n",
    "encode = lambda chars: [stoi[x] for x in chars]\n",
    "decode = lambda ints: [itos[x] for x in ints]\n",
    "\n",
    "test_str = \"From either sides the river lies\"\n",
    "test_str_code = encode(test_str)\n",
    "test_str_decode = decode(test_str_code)\n",
    "print(test_str_code)\n",
    "print(test_str_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和测试集、批量抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire dataset\n",
    "# Split the train and validation set of the dataset\n",
    "# Transfer to tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "split_point = int(0.9 * len(data))\n",
    "train_data = data[:split_point]\n",
    "test_data = data[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[39, 56, 43,  1, 49, 52, 53, 61, 52,  1, 61, 43, 50, 50,  1, 43, 52, 53,\n",
      "         59, 45, 46,  1, 58, 53, 53,  8,  0,  0, 25, 17, 26, 17],\n",
      "        [43, 42, 57,  6,  1, 61, 43,  1, 51, 59, 57, 58,  1, 39, 50, 57, 53,  1,\n",
      "         58, 43, 50, 50,  1, 46, 47, 51,  0, 53, 59, 56,  1, 52],\n",
      "        [57,  1, 56, 43, 39, 57, 53, 52,  1, 61, 47, 58, 46,  1, 46, 47, 51,  8,\n",
      "          0,  0, 15, 24, 13, 30, 17, 26, 15, 17, 10,  0, 35, 46],\n",
      "        [ 8,  0,  0, 26, 59, 56, 57, 43, 10,  0, 13, 52,  1, 46, 53, 52, 53, 59,\n",
      "         56,  2,  1, 61, 43, 56, 43,  1, 52, 53, 58,  1, 21,  1],\n",
      "        [57, 11,  1, 58, 46, 43,  1, 50, 39, 42, 63,  0, 61, 47, 42, 53, 61,  1,\n",
      "         53, 44,  1, 34, 47, 58, 56, 39, 60, 47, 53, 11,  1, 31],\n",
      "        [ 1, 53, 44,  1, 42, 47, 50, 42, 53, 57,  1, 39, 52, 42,  1, 44, 39, 42,\n",
      "         47, 52, 45, 57,  6,  1,  5, 48, 59, 51, 54,  1, 46, 43],\n",
      "        [43, 56, 43,  1, 21,  1, 51, 53, 60, 43,  6,  0, 35, 46, 39, 58,  1, 51,\n",
      "         63,  1, 58, 53, 52, 45, 59, 43,  1, 57, 54, 43, 39, 49],\n",
      "        [61, 47, 42, 43,  1, 45, 39, 54,  6,  1, 57, 47, 52, 41, 43,  1, 47, 58,\n",
      "          1, 47, 57,  1, 47, 52,  1, 51, 63,  1, 54, 53, 61, 43]])\n"
     ]
    }
   ],
   "source": [
    "# Write the get_batch function\n",
    "# Draw chunks of data and understand how to use a chunk of data consider varying length\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_batch(data):\n",
    "    indices = torch.randint(len(data)-block_size, (batch_size, ))\n",
    "    x = [data[start:start+block_size] for start in indices]\n",
    "    y = [data[start+1:start+block_size+1] for start in indices]\n",
    "    x, y = torch.stack(x), torch.stack(y)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(train_data)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Training input is  tensor([18])\n",
      "Target is  tensor(47)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47])\n",
      "Target is  tensor(56)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56])\n",
      "Target is  tensor(57)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57])\n",
      "Target is  tensor(58)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58])\n",
      "Target is  tensor(1)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1])\n",
      "Target is  tensor(15)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15])\n",
      "Target is  tensor(47)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "Target is  tensor(58)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "Target is  tensor(47)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
      "Target is  tensor(64)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64])\n",
      "Target is  tensor(43)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43])\n",
      "Target is  tensor(52)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52])\n",
      "Target is  tensor(10)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10])\n",
      "Target is  tensor(0)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0])\n",
      "Target is  tensor(14)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14])\n",
      "Target is  tensor(43)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43])\n",
      "Target is  tensor(44)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44])\n",
      "Target is  tensor(53)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53])\n",
      "Target is  tensor(56)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56])\n",
      "Target is  tensor(43)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43])\n",
      "Target is  tensor(1)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1])\n",
      "Target is  tensor(61)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61])\n",
      "Target is  tensor(43)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43])\n",
      "Target is  tensor(1)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1])\n",
      "Target is  tensor(54)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54])\n",
      "Target is  tensor(56)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56])\n",
      "Target is  tensor(53)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53])\n",
      "Target is  tensor(41)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41])\n",
      "Target is  tensor(43)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43])\n",
      "Target is  tensor(43)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43])\n",
      "Target is  tensor(42)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42])\n",
      "Target is  tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# The way how a chunk of data is used: Enumerate all possible prediction context length\n",
    "x_example = train_data[:block_size]\n",
    "y_example = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    x = x_example[:t+1]\n",
    "    y = y_example[t]\n",
    "    print(\"----------------------\")\n",
    "    print(\"Training input is \", x)\n",
    "    print(\"Target is \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单BigramLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32, 65])\n",
      "tensor(4.6995, grad_fn=<NllLossBackward0>)\n",
      "un to strike,\n",
      "We'll never leave \n",
      "un to strike,\n",
      "We'll never leave jS!wxEg.\n"
     ]
    }
   ],
   "source": [
    "# Test the data using simple language model BigramLanguageModel\n",
    "# Embedding in Bigram is just a nxn matrix represents the transition probability from token to token\n",
    "\"\"\"\n",
    "BigramLanguageModel\n",
    "-------------------\n",
    "Forward:\n",
    "Input: x, tensor of training data with shape (B, T).\n",
    "Input: target, tensor of labeling data with shape (B, T). Default target = None\n",
    "Output: logits, rows of the probability of each token in the data x, shape (B, T, C), C is the length of the vocab.\n",
    "Output: loss, cross entropy of logits and targets. Note that input logits have to be reshaped to use crossEntropyLoss.\n",
    "\n",
    "Generate: write a generic version that considers the history\n",
    "Input: x, tensor (B, T)\n",
    "Input: max_new_tokens\n",
    "Output: x' after expanding\n",
    "\"\"\"\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        logits = self.token_embedding_table(x)\n",
    "        \n",
    "        if target is None:\n",
    "            loss = 0\n",
    "        else:\n",
    "            B, T, C = logits.shape # C represents channels\n",
    "            logits_reshaped = logits.view(B*T, C)\n",
    "            target = target.view(-1)\n",
    "            loss = F.cross_entropy(logits_reshaped, target)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, predict_len):\n",
    "        for step in range(predict_len):\n",
    "            logits, _ = self(x)\n",
    "            logits = logits[:, -1, :] # Draw the logits of last time step. Now the shape is (B, C)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            char_new = torch.multinomial(probs, 1)\n",
    "            x = torch.cat([x, char_new], dim=1)\n",
    "        return x\n",
    "\n",
    "model = BigramLanguageModel(len(chars))\n",
    "model = model.to(device)\n",
    "x, y = get_batch(train_data)\n",
    "logits, loss = model(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# Use .tolist() method to transform the tensor\n",
    "print(''.join(decode(x[0].tolist())))\n",
    "x_predicted = model.generate(x, 8)\n",
    "print(''.join(decode(x_predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1368.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6132, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Bigram model\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1000\n",
    "for _ in tqdm(range(epochs)):\n",
    "    x, y = get_batch(train_data)\n",
    "    _, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True) # Save memory, and faster\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nd ply his book, welcome his fri\n",
      "nd ply his book, welcome his friKg:p,lFrfjoU?CLJw$SBG,DYet baGrBenml!xgucQig wjZ:VT3va3;kCru-WjZh,?r3;gPBH;E'r\n",
      "yiOGUMc'DMqyoKXIioowy\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(train_data)\n",
    "\n",
    "# Use .tolist() method to transform the tensor\n",
    "print(''.join(decode(x[0].tolist())))\n",
    "x_predicted = model.generate(x, 100)\n",
    "print(''.join(decode(x_predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算attention的向量化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-2.1055,  0.6784, -1.2345],\n",
      "        [-0.0431, -1.6047, -0.7521],\n",
      "        [ 1.6487, -0.3925, -1.4036],\n",
      "        [-0.7279, -0.5594, -0.7688]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# For a (B, T, C) tensor\n",
    "# By averaging over the prefix tokens to summarize the past info and ignore the future info (just like autoregressive)\n",
    "# Ver1: Double loops version\n",
    "# Track the dim can help coding\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "B = 4\n",
    "T = 5\n",
    "C = 3\n",
    "x = torch.randn((B, T, C))\n",
    "\n",
    "print(x[0])\n",
    "\n",
    "x_loop = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        x_loop[b][t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x_loop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# Ver2: Matrix mul version. Efficient, using lower triangle torch.tril(torch.ones(3, 3)) and normalize it\n",
    "# Use torch.allclose() to check these two\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print(wei)\n",
    "x_MM = wei @ x\n",
    "print(x_MM[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# Ver3: Set -inf (it is a mask) and then softmax to get the normalized lower triangular\n",
    "# Why use this? A perspective of affinities. Weight matrix can represent how each token is interesting to each other (In this example it's just 0)\n",
    "# Use [masked_fill] method\n",
    "wei = torch.zeros((T, T))\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, value=-float('inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "x_sm = wei @ x\n",
    "print(x_sm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1491, 0.8509, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6266, 0.0590, 0.3144, 0.0000, 0.0000],\n",
      "        [0.2801, 0.0278, 0.2398, 0.4523, 0.0000],\n",
      "        [0.2760, 0.1401, 0.2008, 0.2031, 0.1801]], grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.5241,  1.4449,  1.0499, -1.2772, -0.4092],\n",
      "        [-0.4937, -0.3647, -1.0037,  1.0804,  0.6383],\n",
      "        [ 0.3009, -0.1853, -0.4107,  1.5519, -1.2764],\n",
      "        [ 0.2153,  1.2557,  0.0874,  0.4091, -1.5597],\n",
      "        [ 0.0270, -0.1853, -0.5358,  1.1992, -0.4808]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.5241,  1.4449,  1.0499, -1.2772, -0.4092],\n",
      "        [-0.3419, -0.0948, -0.6974,  0.7288,  0.4821],\n",
      "        [ 0.3938,  0.8256,  0.4695, -0.2486, -0.6200],\n",
      "        [ 0.3026,  0.9182,  0.2073,  0.2294, -1.1084],\n",
      "        [ 0.1845,  0.5321, -0.0121,  0.4095, -0.6831]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Ver4: Self-attention\n",
    "# query * key to get the weights that represents the affinities(relationship) from token to token\n",
    "# When training, the model will automatically learn the affinities from token to token.\n",
    "# A simple one head attention.\n",
    "\n",
    "\"\"\"\n",
    "Important notes about atttention:\n",
    "1. a communication machanism that can be describe by direct graph with block_size node (the edge represents the affinities.\n",
    "    not necessarily to be a lower triangle. Encoder part needs tokens to talk to each other.\n",
    "2. no notion of space. positional code is needed.\n",
    "3. Raw value x is like private data, and V is like public data for aggregation\n",
    "4. Self-attention: QKV from same source x. Cross-attention: outer source QK.\n",
    "\"\"\"\n",
    "torch.manual_seed(42)\n",
    "head_size = 5\n",
    "Q = nn.Linear(C, head_size)\n",
    "K = nn.Linear(C, head_size)\n",
    "V = nn.Linear(C, head_size)\n",
    "\n",
    "q = Q(x) # (B, T, head_size)\n",
    "k = K(x)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, value=-float('inf')) * head_size**(-0.5)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei[0])\n",
    "\n",
    "v = V(x)\n",
    "print(v[0])\n",
    "x_att = wei @ v # A good way of thinking this mat mul is row perspective\n",
    "print(x_att[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# Why scaling using query and key dimension matters? \n",
    "# We wants the attention to combine more info. If not scaling, it will tend to focus on the largest one.\n",
    "# Essentially, by scalling, the final weight matrix will have low variance.\n",
    "example = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "print(torch.softmax(example, dim=-1))\n",
    "print(torch.softmax(example * 8, dim=-1)) # Not what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Implement a self-attention Head: qkv layer, tril register_buffer, dropout_layer\n",
    "\"\"\"\n",
    "Input: (B, T, embed_size) data, head_size\n",
    "Output: (B, T, head_size)\n",
    "\"\"\"\n",
    "\n",
    "embed_size = 8\n",
    "dropout = 0.1\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_size, head_size)\n",
    "        self.key = nn.Linear(embed_size, head_size)\n",
    "        self.value = nn.Linear(embed_size, head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        wei = q @ k.transpose(-1, -2) * head_size**(-0.5)\n",
    "        print(wei.shape)\n",
    "        print(tril.shape)\n",
    "        wei = wei.masked_fill(tril == 0, value=-float('inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        b = wei @ v # (B, T, head_size)\n",
    "        return b\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "att_head = Head(head_size=8)\n",
    "b = att_head(x)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([4, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([4, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([4, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Implement a multi-head attention with projection (Linear trans)\n",
    "# Projection layer here is to map the size back to embeding_size, which is then compatible for residual connetion\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        b = self.proj(b)\n",
    "        b = self.dropout(b)\n",
    "        return b\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "multi_att_head = MultiHeadAttention(num_heads=4, head_size=embed_size//4)\n",
    "b = multi_att_head(x)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement the feed forward networks after attention layer\n",
    "\"\"\"\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([4, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([4, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([4, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implement attention block class with residual connection\n",
    "- Note that we choose num_heads as hyper parameter instead of choosing head_size. The head_size is computed from embed_size//num_heads\n",
    "? Why choose layer norm? And how to use LayerNorm of 1d and 2d.\n",
    "\"\"\"\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // num_heads       \n",
    "        self.att = MultiHeadAttention(num_heads, head_size)\n",
    "        self.fc = FeedForward(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.ln1(self.att(x))\n",
    "        x = x + self.ln2(self.fc(x))\n",
    "        return x\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "block = Block(embed_size, num_heads=4)\n",
    "print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention用于语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modification on BigramModel -- 3 Linear layers\n",
    "1. token_embedding layer (vocab_size, embed_size)\n",
    "2. position_embedding layer (block_size, embed_size)\n",
    "3. language model head used to compute logits (embed_size, block_size)\n",
    "4. Pluggin the attention block (use multi-head directly)\n",
    "5. Pluggin the last feedforward layer\n",
    "\n",
    "Make sure you understand the embedding here, which is no longer the same as original BigrameModel (which represents the transition probability)\n",
    "    nn.Embedding creates a lookup table that converts indices (usually token IDs) into dense vectors of fixed size. \n",
    "    It's commonly used as the first layer in NLP tasks to convert tokens to continuous representations.\n",
    "So this model is actually not a BigramModel anymore.\n",
    "\n",
    "Just to remind1: loss is computed through cross_entropy. before using F.cross_entropy, you only need to reshape the logits and target.\n",
    "Just to remind2: use F.softmax to convert the logits to the probability and then use torch.multinomial to predict the next token.\n",
    "\n",
    "? Why token_embed and pos_embed use nn.Embedding instead of nn.Linear\n",
    "? Understand the embedding in NLP tasks, why it is necessary.\n",
    "\"\"\"\n",
    "\n",
    "num_layers = 6\n",
    "num_heads = 4\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embed = nn.Embedding(block_size, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block(embed_size, num_heads) for _ in range(num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok_emb = self.token_embed(x) # (B, T, C)\n",
    "        pos_emb = self.pos_embed(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits_temp = logits.reshape(B*T, C)\n",
    "            target_temp = target.reshape(B*T)\n",
    "            loss = F.cross_entropy(logits_temp, target_temp)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, x, num_predict):\n",
    "        for t in range(num_predict):\n",
    "            x_cond = x[:,-block_size:] # (B, T)\n",
    "            logits, _ = self(x_cond) # (B, T, vocab_size)\n",
    "            logits = logits[:,-1,:] # (B, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
    "            x_next = torch.multinomial(probs, 1) # (B, 1)\n",
    "            x = torch.cat([x, x_next], dim=1) # (B, T+1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最后测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Func for train and eval the model.\n",
    "Use random batch from train and eval dataset to evaluate the mean losses.\n",
    "Output: list, contains two scalars represent the 'train' and 'eval' losses.\n",
    "\"\"\"\n",
    "\n",
    "eval_iters = 200\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = []\n",
    "    model.eval()\n",
    "    for split in [train_data, test_data]: # This is not efficient!! Thats why we would prefer using string to choose the dataset\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss\n",
    "        out.append(losses.mean())     \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 32])\n",
      "torch.Size([5, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (32) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m eval_iters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m it \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(train_data)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[176], line 17\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m     16\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m---> 17\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     19\u001b[0m out\u001b[38;5;241m.\u001b[39mappend(losses\u001b[38;5;241m.\u001b[39mmean())     \n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[175], line 39\u001b[0m, in \u001b[0;36mBigramModel.forward\u001b[1;34m(self, x, target)\u001b[0m\n\u001b[0;32m     37\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;66;03m# (T, C)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[1;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x) \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x) \u001b[38;5;66;03m# (B, T, vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[174], line 17\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x))\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[172], line 11\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(b)\n\u001b[0;32m     13\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(b)\n",
      "Cell \u001b[1;32mIn[172], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(b)\n\u001b[0;32m     13\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(b)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[171], line 26\u001b[0m, in \u001b[0;36mHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(wei\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(tril\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 26\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[43mwei\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtril\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(wei)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (32) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_iters = 200\n",
    "lr = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embed_size = 64\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "dropout = 0.0\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "model = BigramModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for it in range(max_iters):\n",
    "    if it % eval_iters == 0 or it == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step{it}, Train loss: {losses[0]}, Val loss: {losses[1]}')\n",
    "\n",
    "    X, Y = get_batch(train_data)\n",
    "    logits, loss = model(X, Y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
