{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of chars 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"num of chars\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# First 1000 chars\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# All unique chars that occur\n",
    "# We'all use char as token\n",
    "chars = sorted(list(set(text)))\n",
    "print(''.join(chars))\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 56, 53, 51, 1, 43, 47, 58, 46, 43, 56, 1, 57, 47, 42, 43, 57, 1, 58, 46, 43, 1, 56, 47, 60, 43, 56, 1, 50, 47, 43, 57]\n",
      "['F', 'r', 'o', 'm', ' ', 'e', 'i', 't', 'h', 'e', 'r', ' ', 's', 'i', 'd', 'e', 's', ' ', 't', 'h', 'e', ' ', 'r', 'i', 'v', 'e', 'r', ' ', 'l', 'i', 'e', 's']\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenizer example (just using index) (More complex, see OpenAI tiktoken)\n",
    "# Write the encode and decode function\n",
    "stoi = {character:index for index, character in enumerate(chars)}\n",
    "itos = {index:character for index, character in enumerate(chars)}\n",
    "encode = lambda chars: [stoi[x] for x in chars]\n",
    "decode = lambda ints: [itos[x] for x in ints]\n",
    "\n",
    "test_str = \"From either sides the river lies\"\n",
    "test_str_code = encode(test_str)\n",
    "test_str_decode = decode(test_str_code)\n",
    "print(test_str_code)\n",
    "print(test_str_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire dataset\n",
    "# Split the train and validation set of the dataset\n",
    "# Transfer to tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "split_point = int(0.9 * len(data))\n",
    "train_data = data[:split_point]\n",
    "test_data = data[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[58, 43, 50, 63,  1, 51, 53, 56],\n",
      "        [ 0, 40, 63,  1, 51, 47, 52, 43],\n",
      "        [ 0, 20, 53, 61,  1, 41, 53, 59],\n",
      "        [43, 56, 57, 58, 39, 52, 42, 47],\n",
      "        [47, 52,  1, 53, 59, 56,  1, 44],\n",
      "        [42,  1, 54, 56, 43, 57, 39, 45],\n",
      "        [53, 59, 56,  8,  0, 37, 53, 59],\n",
      "        [ 1, 39, 57,  1, 39, 52, 63,  1]])\n"
     ]
    }
   ],
   "source": [
    "# Write the get_batch function\n",
    "# Draw chunks of data and understand how to use a chunk of data consider varying length\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_batch(data):\n",
    "    indices = torch.randint(len(data)-block_size, (batch_size, ))\n",
    "    x = [data[start:start+block_size] for start in indices]\n",
    "    y = [data[start+1:start+block_size+1] for start in indices]\n",
    "    x, y = torch.stack(x), torch.stack(y)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(train_data)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Training input is  tensor([18])\n",
      "Target is  tensor(47)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47])\n",
      "Target is  tensor(56)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56])\n",
      "Target is  tensor(57)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57])\n",
      "Target is  tensor(58)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58])\n",
      "Target is  tensor(1)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1])\n",
      "Target is  tensor(15)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15])\n",
      "Target is  tensor(47)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "Target is  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "# The way how a chunk of data is used: Enumerate all possible prediction context length\n",
    "x_example = train_data[:block_size]\n",
    "y_example = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    x = x_example[:t+1]\n",
    "    y = y_example[t]\n",
    "    print(\"----------------------\")\n",
    "    print(\"Training input is \", x)\n",
    "    print(\"Target is \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 65])\n",
      "tensor(4.8997, grad_fn=<NllLossBackward0>)\n",
      "girl.\n",
      "\n",
      "J\n",
      "girl.\n",
      "\n",
      "JlpkRcoiy\n"
     ]
    }
   ],
   "source": [
    "# Test the data using simple language model BigramLanguageModel\n",
    "# Embedding in Bigram is just a nxn matrix represents the transition probability from token to token\n",
    "\"\"\"\n",
    "BigramLanguageModel\n",
    "-------------------\n",
    "Forward:\n",
    "Input: x, tensor of training data with shape (B, T).\n",
    "Input: target, tensor of labeling data with shape (B, T). Default target = None\n",
    "Output: logits, rows of the probability of each token in the data x, shape (B, T, C), C is the length of the vocab.\n",
    "Output: loss, cross entropy of logits and targets. Note that input logits have to be reshaped to use crossEntropyLoss.\n",
    "\n",
    "Generate: write a generic version that considers the history\n",
    "Input: x, tensor (B, T)\n",
    "Input: max_new_tokens\n",
    "Output: x' after expanding\n",
    "\"\"\"\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        logits = self.token_embedding_table(x)\n",
    "        \n",
    "        if target is None:\n",
    "            loss = 0\n",
    "        else:\n",
    "            B, T, C = logits.shape # C represents channels\n",
    "            logits_reshaped = logits.view(B*T, C)\n",
    "            target = target.view(-1)\n",
    "            loss = F.cross_entropy(logits_reshaped, target)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, predict_len):\n",
    "        for step in range(predict_len):\n",
    "            logits, _ = self(x)\n",
    "            logits = logits[:, -1, :] # Draw the logits of last time step. Now the shape is (B, C)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            char_new = torch.multinomial(probs, 1)\n",
    "            x = torch.cat([x, char_new], dim=1)\n",
    "        return x\n",
    "\n",
    "model = BigramLanguageModel(len(chars))\n",
    "model = model.to(device)\n",
    "x, y = get_batch(train_data)\n",
    "logits, loss = model(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# Use .tolist() method to transform the tensor\n",
    "print(''.join(decode(x[0].tolist())))\n",
    "x_predicted = model.generate(x, 8)\n",
    "print(''.join(decode(x_predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:06<00:00, 1535.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6425, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Bigram model\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1000\n",
    "for _ in tqdm(range(epochs)):\n",
    "    x, y = get_batch(train_data)\n",
    "    _, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True) # Save memory, and faster\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff, I ha\n",
      "ff, I hau\n",
      "NEOTold!Ys'te d\n",
      "CHQPA: D: fr;\n",
      "AUKENCUp? wht tei'\n",
      "\n",
      "Aupenerisonene:\n",
      "ALLICr cac andr cal\n",
      "\n",
      "wrhesay d n\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(train_data)\n",
    "\n",
    "# Use .tolist() method to transform the tensor\n",
    "print(''.join(decode(x[0].tolist())))\n",
    "x_predicted = model.generate(x, 100)\n",
    "print(''.join(decode(x_predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算attention的向量化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-2.1055,  0.6784, -1.2345],\n",
      "        [-0.0431, -1.6047, -0.7521],\n",
      "        [ 1.6487, -0.3925, -1.4036]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224]])\n"
     ]
    }
   ],
   "source": [
    "# For a (B, T, C) tensor\n",
    "# By averaging over the prefix tokens to summarize the past info and ignore the future info (just like autoregressive)\n",
    "# Ver1: Double loops version\n",
    "# Track the dim can help coding\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "B = 4\n",
    "T = 4\n",
    "C = 3\n",
    "x = torch.randn((B, T, C))\n",
    "\n",
    "print(x[0])\n",
    "\n",
    "x_loop = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        x_loop[b][t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x_loop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224]])\n"
     ]
    }
   ],
   "source": [
    "# Ver2: Matrix mul version. Efficient, using lower triangle torch.tril(torch.ones(3, 3)) and normalize it\n",
    "# Use torch.allclose() to check these two\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print(wei)\n",
    "x_MM = wei @ x\n",
    "print(x_MM[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224]])\n"
     ]
    }
   ],
   "source": [
    "# Ver3: Set -inf (it is a mask) and then softmax to get the normalized lower triangular\n",
    "# Why use this? A perspective of affinities. Weight matrix can represent how each token is interesting to each other (In this example it's just 0)\n",
    "# Use [masked_fill] method\n",
    "wei = torch.zeros((T, T))\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, value=-float('inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "x_sm = wei @ x\n",
    "print(x_sm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1491, 0.8509, 0.0000, 0.0000],\n",
      "        [0.6266, 0.0590, 0.3144, 0.0000],\n",
      "        [0.2801, 0.0278, 0.2398, 0.4523]], grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.5241,  1.4449,  1.0499, -1.2772, -0.4092],\n",
      "        [-0.4937, -0.3647, -1.0037,  1.0804,  0.6383],\n",
      "        [ 0.3009, -0.1853, -0.4107,  1.5519, -1.2764],\n",
      "        [ 0.2153,  1.2557,  0.0874,  0.4091, -1.5597]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.5241,  1.4449,  1.0499, -1.2772, -0.4092],\n",
      "        [-0.3419, -0.0948, -0.6974,  0.7288,  0.4821],\n",
      "        [ 0.3938,  0.8256,  0.4695, -0.2486, -0.6200],\n",
      "        [ 0.3026,  0.9182,  0.2073,  0.2294, -1.1084]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Ver4: Self-attention\n",
    "# query * key to get the weights that represents the affinities(relationship) from token to token\n",
    "# When training, the model will automatically learn the affinities from token to token.\n",
    "# A simple one head attention.\n",
    "\n",
    "\"\"\"\n",
    "Important notes about atttention:\n",
    "1. a communication machanism that can be describe by direct graph with block_size node (the edge represents the affinities.\n",
    "    not necessarily to be a lower triangle. Encoder part needs tokens to talk to each other.\n",
    "2. no notion of space. positional code is needed.\n",
    "3. Raw value x is like private data, and V is like public data for aggregation\n",
    "4. Self-attention: QKV from same source x. Cross-attention: outer source QK.\n",
    "\"\"\"\n",
    "torch.manual_seed(42)\n",
    "head_size = 5\n",
    "Q = nn.Linear(C, head_size)\n",
    "K = nn.Linear(C, head_size)\n",
    "V = nn.Linear(C, head_size)\n",
    "\n",
    "q = Q(x) # (B, T, head_size)\n",
    "k = K(x)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, value=-float('inf')) * head_size**(-0.5)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei[0])\n",
    "\n",
    "v = V(x)\n",
    "print(v[0])\n",
    "x_att = wei @ v # A good way of thinking this mat mul is row perspective\n",
    "print(x_att[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# Why scaling using query and key dimension matters? \n",
    "# We wants the attention to combine more info. If not scaling, it will tend to focus on the largest one.\n",
    "# Essentially, by scalling, the final weight matrix will have low variance.\n",
    "example = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "print(torch.softmax(example, dim=-1))\n",
    "print(torch.softmax(example * 8, dim=-1)) # Not what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a self-attention Head\n",
    "# 'tril' pytorch naming convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a multi-head attention with projection (Linear trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nModification on BigramModel -- 3 Linear layers\\n1. token_embedding layer (vocab_size, embed_size)\\n2. position_embedding layer (block_size, embed_size)\\n3. language model head used to compute logits (embed_size, block_size)\\n4. Pluggin the attention block (use multi-head directly)\\n5. Pluggin the last feedforward layer\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Modification on BigramModel -- 3 Linear layers\n",
    "1. token_embedding layer (vocab_size, embed_size)\n",
    "2. position_embedding layer (block_size, embed_size)\n",
    "3. language model head used to compute logits (embed_size, block_size)\n",
    "4. Pluggin the attention block (use multi-head directly)\n",
    "5. Pluggin the last feedforward layer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImplement attention block class with residual connection\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implement attention block class with residual connection\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
