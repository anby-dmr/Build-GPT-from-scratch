{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of chars 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"num of chars\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# First 1000 chars\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# All unique chars that occur\n",
    "# We'all use char as token\n",
    "chars = sorted(list(set(text)))\n",
    "print(''.join(chars))\n",
    "print(len(chars))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 56, 53, 51, 1, 43, 47, 58, 46, 43, 56, 1, 57, 47, 42, 43, 57, 1, 58, 46, 43, 1, 56, 47, 60, 43, 56, 1, 50, 47, 43, 57]\n",
      "['F', 'r', 'o', 'm', ' ', 'e', 'i', 't', 'h', 'e', 'r', ' ', 's', 'i', 'd', 'e', 's', ' ', 't', 'h', 'e', ' ', 'r', 'i', 'v', 'e', 'r', ' ', 'l', 'i', 'e', 's']\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenizer example (just using index) (More complex, see OpenAI tiktoken)\n",
    "# Write the encode and decode function\n",
    "stoi = {character:index for index, character in enumerate(chars)}\n",
    "itos = {index:character for index, character in enumerate(chars)}\n",
    "encode = lambda chars: [stoi[x] for x in chars]\n",
    "decode = lambda ints: [itos[x] for x in ints]\n",
    "\n",
    "test_str = \"From either sides the river lies\"\n",
    "test_str_code = encode(test_str)\n",
    "test_str_decode = decode(test_str_code)\n",
    "print(test_str_code)\n",
    "print(test_str_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和测试集、批量抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire dataset\n",
    "# Split the train and validation set of the dataset\n",
    "# Transfer to tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "split_point = int(0.9 * len(data))\n",
    "train_data = data[:split_point]\n",
    "test_data = data[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[45,  1, 61, 47, 58, 46,  1, 46],\n",
      "        [53, 52, 43,  6,  0, 35, 43, 56],\n",
      "        [ 1, 21, 21, 21, 10,  0, 20, 53],\n",
      "        [58, 46, 47, 57,  1, 47, 57,  1],\n",
      "        [49,  6,  1, 40, 43,  1, 49, 47],\n",
      "        [ 1, 58, 46, 43,  1, 45, 53, 53],\n",
      "        [43, 63,  1, 46, 39, 60, 43,  1],\n",
      "        [ 1, 39, 52,  1, 46, 53, 59, 56]])\n"
     ]
    }
   ],
   "source": [
    "# Write the get_batch function\n",
    "# Draw chunks of data and understand how to use a chunk of data consider varying length\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_batch(data):\n",
    "    indices = torch.randint(len(data)-block_size, (batch_size, ))\n",
    "    x = [data[start:start+block_size] for start in indices]\n",
    "    y = [data[start+1:start+block_size+1] for start in indices]\n",
    "    x, y = torch.stack(x), torch.stack(y)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(train_data)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Training input is  tensor([18])\n",
      "Target is  tensor(47)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47])\n",
      "Target is  tensor(56)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56])\n",
      "Target is  tensor(57)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57])\n",
      "Target is  tensor(58)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58])\n",
      "Target is  tensor(1)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1])\n",
      "Target is  tensor(15)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15])\n",
      "Target is  tensor(47)\n",
      "----------------------\n",
      "Training input is  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "Target is  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "# The way how a chunk of data is used: Enumerate all possible prediction context length\n",
    "x_example = train_data[:block_size]\n",
    "y_example = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    x = x_example[:t+1]\n",
    "    y = y_example[t]\n",
    "    print(\"----------------------\")\n",
    "    print(\"Training input is \", x)\n",
    "    print(\"Target is \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单BigramLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 65])\n",
      "tensor(4.5735, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "YORK:\n",
      "T\n",
      "\n",
      "YORK:\n",
      "Tcsefflo3\n"
     ]
    }
   ],
   "source": [
    "# Test the data using simple language model BigramLanguageModel\n",
    "# Embedding in Bigram is just a nxn matrix represents the transition probability from token to token\n",
    "\"\"\"\n",
    "BigramLanguageModel\n",
    "-------------------\n",
    "Forward:\n",
    "Input: x, tensor of training data with shape (B, T).\n",
    "Input: target, tensor of labeling data with shape (B, T). Default target = None\n",
    "Output: logits, rows of the probability of each token in the data x, shape (B, T, C), C is the length of the vocab.\n",
    "Output: loss, cross entropy of logits and targets. Note that input logits have to be reshaped to use crossEntropyLoss.\n",
    "\n",
    "Generate: write a generic version that considers the history\n",
    "Input: x, tensor (B, T)\n",
    "Input: max_new_tokens\n",
    "Output: x' after expanding\n",
    "\"\"\"\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        logits = self.token_embedding_table(x)\n",
    "        \n",
    "        if target is None:\n",
    "            loss = 0\n",
    "        else:\n",
    "            B, T, C = logits.shape # C represents channels\n",
    "            logits_reshaped = logits.view(B*T, C)\n",
    "            target = target.view(-1)\n",
    "            loss = F.cross_entropy(logits_reshaped, target)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, predict_len):\n",
    "        for step in range(predict_len):\n",
    "            logits, _ = self(x)\n",
    "            logits = logits[:, -1, :] # Draw the logits of last time step. Now the shape is (B, C)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            char_new = torch.multinomial(probs, 1)\n",
    "            x = torch.cat([x, char_new], dim=1)\n",
    "        return x\n",
    "\n",
    "model = BigramLanguageModel(len(chars))\n",
    "model = model.to(device)\n",
    "x, y = get_batch(train_data)\n",
    "logits, loss = model(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# Use .tolist() method to transform the tensor\n",
    "print(''.join(decode(x[0].tolist())))\n",
    "x_predicted = model.generate(x, 8)\n",
    "print(''.join(decode(x_predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1554.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5961, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Bigram model\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1000\n",
    "for _ in tqdm(range(epochs)):\n",
    "    x, y = get_batch(train_data)\n",
    "    _, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True) # Save memory, and faster\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your nam\n",
      "your nam m, m, and fil'de cu t be now tor m l ma n w k wind wiss 'llouse hailee,Mheno foounlle r irechal m o\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(train_data)\n",
    "\n",
    "# Use .tolist() method to transform the tensor\n",
    "print(''.join(decode(x[0].tolist())))\n",
    "x_predicted = model.generate(x, 100)\n",
    "print(''.join(decode(x_predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算attention的向量化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-2.1055,  0.6784, -1.2345],\n",
      "        [-0.0431, -1.6047, -0.7521],\n",
      "        [ 1.6487, -0.3925, -1.4036],\n",
      "        [-0.7279, -0.5594, -0.7688]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# For a (B, T, C) tensor\n",
    "# By averaging over the prefix tokens to summarize the past info and ignore the future info (just like autoregressive)\n",
    "# Ver1: Double loops version\n",
    "# Track the dim can help coding\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "B = 4\n",
    "T = 5\n",
    "C = 3\n",
    "x = torch.randn((B, T, C))\n",
    "\n",
    "print(x[0])\n",
    "\n",
    "x_loop = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        x_loop[b][t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x_loop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# Ver2: Matrix mul version. Efficient, using lower triangle torch.tril(torch.ones(3, 3)) and normalize it\n",
    "# Use torch.allclose() to check these two\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print(wei)\n",
    "x_MM = wei @ x\n",
    "print(x_MM[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-0.0893,  1.0829, -0.1669],\n",
      "        [-0.0739,  0.1870, -0.3620],\n",
      "        [ 0.3568,  0.0421, -0.6224],\n",
      "        [ 0.1398, -0.0782, -0.6517]])\n"
     ]
    }
   ],
   "source": [
    "# Ver3: Set -inf (it is a mask) and then softmax to get the normalized lower triangular\n",
    "# Why use this? A perspective of affinities. Weight matrix can represent how each token is interesting to each other (In this example it's just 0)\n",
    "# Use [masked_fill] method\n",
    "wei = torch.zeros((T, T))\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, value=-float('inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "x_sm = wei @ x\n",
    "print(x_sm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1491, 0.8509, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6266, 0.0590, 0.3144, 0.0000, 0.0000],\n",
      "        [0.2801, 0.0278, 0.2398, 0.4523, 0.0000],\n",
      "        [0.2760, 0.1401, 0.2008, 0.2031, 0.1801]], grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.5241,  1.4449,  1.0499, -1.2772, -0.4092],\n",
      "        [-0.4937, -0.3647, -1.0037,  1.0804,  0.6383],\n",
      "        [ 0.3009, -0.1853, -0.4107,  1.5519, -1.2764],\n",
      "        [ 0.2153,  1.2557,  0.0874,  0.4091, -1.5597],\n",
      "        [ 0.0270, -0.1853, -0.5358,  1.1992, -0.4808]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.5241,  1.4449,  1.0499, -1.2772, -0.4092],\n",
      "        [-0.3419, -0.0948, -0.6974,  0.7288,  0.4821],\n",
      "        [ 0.3938,  0.8256,  0.4695, -0.2486, -0.6200],\n",
      "        [ 0.3026,  0.9182,  0.2073,  0.2294, -1.1084],\n",
      "        [ 0.1845,  0.5321, -0.0121,  0.4095, -0.6831]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Ver4: Self-attention\n",
    "# query * key to get the weights that represents the affinities(relationship) from token to token\n",
    "# When training, the model will automatically learn the affinities from token to token.\n",
    "# A simple one head attention.\n",
    "\n",
    "\"\"\"\n",
    "Important notes about atttention:\n",
    "1. a communication machanism that can be describe by direct graph with block_size node (the edge represents the affinities.\n",
    "    not necessarily to be a lower triangle. Encoder part needs tokens to talk to each other.\n",
    "2. no notion of space. positional code is needed.\n",
    "3. Raw value x is like private data, and V is like public data for aggregation\n",
    "4. Self-attention: QKV from same source x. Cross-attention: outer source QK.\n",
    "\"\"\"\n",
    "torch.manual_seed(42)\n",
    "head_size = 5\n",
    "Q = nn.Linear(C, head_size)\n",
    "K = nn.Linear(C, head_size)\n",
    "V = nn.Linear(C, head_size)\n",
    "\n",
    "q = Q(x) # (B, T, head_size)\n",
    "k = K(x)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, value=-float('inf')) * head_size**(-0.5)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei[0])\n",
    "\n",
    "v = V(x)\n",
    "print(v[0])\n",
    "x_att = wei @ v # A good way of thinking this mat mul is row perspective\n",
    "print(x_att[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# Why scaling using query and key dimension matters? \n",
    "# We wants the attention to combine more info. If not scaling, it will tend to focus on the largest one.\n",
    "# Essentially, by scalling, the final weight matrix will have low variance.\n",
    "example = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "print(torch.softmax(example, dim=-1))\n",
    "print(torch.softmax(example * 8, dim=-1)) # Not what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Implement a self-attention Head: qkv layer, tril register_buffer, dropout_layer\n",
    "\"\"\"\n",
    "Input: (B, T, embed_size) data, head_size\n",
    "Output: (B, T, head_size)\n",
    "\n",
    "- Note that we add dropout for weight matrix to randomly prevent some interaction.\n",
    "\n",
    "? Why use C**(-0.5) instead of head_size**(-0.5)? I think the head_size it the right choice.\n",
    "? Why slicing the tril matrix\n",
    "\"\"\"\n",
    "\n",
    "embed_size = 8\n",
    "dropout = 0.1\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False) # qkv usually not use bias.\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        wei = q @ k.transpose(-1, -2) * self.head_size**(-0.5) # Guess this should be head_size instead of C according to the paper.\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, value=-float('inf')) # Careful! Slice the tril. Because during inference, the length of data may change.\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        b = wei @ v # (B, T, head_size)\n",
    "        return b\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "att_head = Head(head_size=8)\n",
    "b = att_head(x)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Implement a multi-head attention with projection (Linear trans)\n",
    "# Projection layer here is to map the size back to embeding_size, which is then compatible for residual connetion\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        b = self.proj(b)\n",
    "        b = self.dropout(b)\n",
    "        return b\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "multi_att_head = MultiHeadAttention(num_heads=4, head_size=embed_size//4)\n",
    "b = multi_att_head(x)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement the feed forward networks after attention layer\n",
    "- Note that we add dropout before the res connection.\n",
    "\"\"\"\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implement attention block class with residual connection\n",
    "- Note that we choose num_heads as hyper parameter instead of choosing head_size. The head_size is computed from embed_size//num_heads\n",
    "- Note that we usually use layerNorm before att and fc now, which is opposite from the original paper.\n",
    "? Why choose layer norm? And how to use LayerNorm of 1d and 2d.\n",
    "\"\"\"\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // num_heads       \n",
    "        self.att = MultiHeadAttention(num_heads, head_size)\n",
    "        self.fc = FeedForward(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.fc(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "block = Block(embed_size, num_heads=4)\n",
    "print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention组件 - 并行版\n",
    "把多头注意力的计算过程合并成一个矩阵运算，以此利用GPU并行能力，加速计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention_Fast(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.d_model = num_heads * head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.Q = nn.Linear(self.d_model, self.d_model)\n",
    "        self.K = nn.Linear(self.d_model, self.d_model)\n",
    "        self.V = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "        self.proj = nn.Linear(self.d_model, self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        # compute all heads at one time\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "\n",
    "        # split into different heads\n",
    "        # (B, T, #heads, head_size)\n",
    "        q = q.view(B, T, self.num_heads, self.head_size) \n",
    "        k = k.view(B, T, self.num_heads, self.head_size)\n",
    "        v = v.view(B, T, self.num_heads, self.head_size)\n",
    "\n",
    "        # transpose for further computation\n",
    "        # (B, #heads, T, head_size)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # masked scaled dot attention\n",
    "        wei = q @ k.transpose(-1, -2) * self.head_size ** (-0.5) # (B, #heads, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, value=-float('inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        # compute the combined value b, transpose back\n",
    "        b = wei @ v # (B, #heads, T, head_size)\n",
    "        b = b.transpose(1, 2) # (B, T, #heads, head_size)\n",
    "        b = b.reshape(B, T, -1) # (B, T, d_model)\n",
    "\n",
    "        # proj and dropout\n",
    "        b = self.proj(b)\n",
    "        b = self.dropout(b)\n",
    "        return b\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "multi_fast = MultiHeadAttention_Fast(num_heads=4, head_size=embed_size // 4)\n",
    "b = multi_fast(x)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "class Block_Fast(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // num_heads       \n",
    "        self.att = MultiHeadAttention_Fast(num_heads, head_size)\n",
    "        self.fc = FeedForward(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.fc(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "x = torch.randn((B, T, embed_size))\n",
    "block = Block(embed_size, num_heads=4)\n",
    "print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention用于语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modification on BigramModel -- 3 Linear layers\n",
    "1. token_embedding layer (vocab_size, embed_size)\n",
    "2. position_embedding layer (block_size, embed_size)\n",
    "3. language model head used to compute logits (embed_size, block_size)\n",
    "4. Pluggin the attention block (use multi-head directly)\n",
    "5. Pluggin the last feedforward layer\n",
    "\n",
    "Make sure you understand the embedding here, which is no longer the same as original BigrameModel (which represents the transition probability)\n",
    "    nn.Embedding creates a lookup table that converts indices (usually token IDs) into dense vectors of fixed size. \n",
    "    It's commonly used as the first layer in NLP tasks to convert tokens to continuous representations.\n",
    "So this model is actually not a BigramModel anymore.\n",
    "\n",
    "Just to remind1: loss is computed through cross_entropy. before using F.cross_entropy, you only need to reshape the logits and target.\n",
    "Just to remind2: use F.softmax to convert the logits to the probability and then use torch.multinomial to predict the next token.\n",
    "\n",
    "? Why token_embed and pos_embed use nn.Embedding instead of nn.Linear\n",
    "? Understand the embedding in NLP tasks, why it is necessary.\n",
    "\"\"\"\n",
    "\n",
    "num_layers = 6\n",
    "num_heads = 4\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embed = nn.Embedding(block_size, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block_Fast(embed_size, num_heads) for _ in range(num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok_emb = self.token_embed(x) # (B, T, C)\n",
    "        pos_emb = self.pos_embed(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits_temp = logits.reshape(B*T, C)\n",
    "            target_temp = target.reshape(B*T)\n",
    "            loss = F.cross_entropy(logits_temp, target_temp)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, x, num_predict):\n",
    "        for t in range(num_predict):\n",
    "            x_cond = x[:,-block_size:] # (B, T)\n",
    "            logits, _ = self(x_cond) # (B, T, vocab_size)\n",
    "            logits = logits[:,-1,:] # (B, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
    "            x_next = torch.multinomial(probs, 1) # (B, 1)\n",
    "            x = torch.cat([x, x_next], dim=1) # (B, T+1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context test after training in gpt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.210497 M params\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_iters = 200\n",
    "lr = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embed_size = 64\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "dropout = 0.0\n",
    "\n",
    "model = BigramModel()\n",
    "model = model.to(device)\n",
    "# Print the num of parameters\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M params')\n",
    "model.load_state_dict(torch.load('GPT_faster_attention4999.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With to they find sad with the may, our fulbles\n",
      "I way nusband bagen up on\n",
      "And duke; appossext face, curge Mercer purden; with a he\n",
      "hath, soft willnger?\n",
      "\n",
      "DUKE OF YORK:\n",
      "Ah the cuse than they cansue.\n",
      "-heal made men; to him make who!\n",
      "Cance a part weers makest in stall of the lige:\n",
      "He every foul inale, pueachines: my lords main; noble?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "O you lord, you may of new yeart;\n",
      "you no must office, I am brancher\n",
      "gone of they, thangamer an pick: my lords, so you\n",
      "Our what make struings his been\n",
      "evenes br, take him I have day! Balounds as\n",
      "very, to didnepss and mean I treven Englened.\n",
      "Whose a nimine shall burine chame\n",
      "Which, thrie, make keman.\n",
      "\n",
      "PAULINA:\n",
      "I Rigold Enaloughings him noble you.\n",
      "\n",
      "GRUSER.\n",
      "CLIOND:\n",
      "A one the endemper.\n",
      "Aput yet full your every with sensundet are thou like, pear not on, thisBiny;\n",
      "Who the hour sups fivilaintent than you, that let take Zome, sent! him.\n",
      "\n",
      "Prover:\n",
      "Could nung fillow; I renainting;\n",
      "My our let, be thank it theseful howard: as my lords! I am will deak your may so\n",
      "blear-should cuseny will thou wises, the now your fathert consarge!\n",
      "What maht eyeshuritune, a way, for may my\n",
      "it park with our haves grace acass onp\n",
      "Your profortarning lameas of accame welp man I am him nature:\n",
      "I head the plecues is for very men\n",
      "Of in our beach encl-mine.\n",
      "\n",
      "Lord Masty him save himing-seem to reases:\n",
      "And dones, as I scal name an ope head,\n",
      "And be and feeper-gate a with a thin plevens the came\n",
      "Bugh his of him knock like owe crace\n",
      "would her I suay of forst\n",
      "Before this, he will you; Came my ary bellings,\n",
      "The comp that I un&er my lord shone you.\n",
      "\n",
      "BENVOLIO:\n",
      "Then straints a should mastinus\n",
      "Eng--him lorgy's estant must-stack the some\n",
      "Of thalm go me hear hiness;\n",
      "Out storm yes a keed Mucy nomer of flrmath\n",
      "And the shall look out EnIbble.\n",
      "\n",
      "CLAUDIO:\n",
      "Nurs, ank are first me to the speperried o' times,\n",
      "No think, singman: his Flearh you.\n",
      "Why, mragin, comey, my lave the strift ment: your suche healted.\n",
      "\n",
      "DUKE ANTIO:\n",
      "The lust in our churring bird a this some unbrobbanged, nother,\n",
      "Sh\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # Input (B=1, T=1)\n",
    "print(''.join(decode(model.generate(context, 2000)[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试实现LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更多细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add dropout and layerNorm only when you're ready to scal up the model to prevent overfitting.\n",
    "2. Why decoder only? Because we perform non-conditional generation. No needs for encoder. (The original paper perform machine translation, which is a conditional generation task)\n",
    "3. Mask in decoder make the model autoregressive.\n",
    "4. A more efficient way: Causal self-attention, which incorporate the multi-head into a dimension of the data, making it (B, T, C, H).\n",
    "5. Two phases of training a LLM: (1) Document completer: pre-train on large internet data (just like what we did here). (2) QA assistant: fine-tuning on the data of QA format, and train a reward model then do RLHF. This stage is harder and a lot of data is closed-source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
